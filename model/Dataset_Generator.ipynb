{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOvew+Y4C7kWIXvsJunk+Iz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":5,"metadata":{"id":"ShOF5xFYa-Ue","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678464035324,"user_tz":-480,"elapsed":3121,"user":{"displayName":"Kuro Neko","userId":"15297484354292566686"}},"outputId":"f8d30f0a-51d5-4faf-e435-b5bca36ea06b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# This notebook seres as the code to generate the dataset that will be used to build the Semantic Search Model as well as the Recommender model\n","# It is best to run this file every new season of anime or once every month to update the anime to the most current\n","# This data comes from the jikan.moe.api \n","\n","# Mount Google Drive (Because i am using Google Colab)\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# Lets Load the dataset from jikan.moe.api\n","import requests\n","import json\n","import datetime\n","import tqdm\n","import time\n","import os\n","\n","# We will take the top anime\n","api_url = 'https://api.jikan.moe/v4/top/anime'\n","\n","ourdata = []\n","# Not All data is needed for the content based recommender system,Thus lets only keep the fields that are neccesary\n","csvheader = ['MAL ID','Title','Medium Type','Original Source','Status','Rating','Score','ScoredBy','Members','Favourite','Synopsis','Producers','Studios','Genres','Themes','Demographic']\n","\n","def get_data(page):\n","  response = requests.get(api_url+f'?page={page}')\n","  response.raise_for_status()\n","  data = response.json()\n","  for x in data['data']:\n","      # From the jikan docs these are the fields that have a list of items\n","      genres_name = [y['name'] for y in x['genres']]\n","      producer_name = [y['name'] for y in x['producers']]\n","      studios_name = [y['name'] for y in x['studios']]\n","      themes_name = [y['name'] for y in x['themes']]\n","      demographics_name = [y['name'] for y in x['demographics']]\n","      content = [\n","                x['mal_id'],\n","                x['title'],\n","                x['type'],\n","                x['source'],\n","                x['status'],\n","                x['rating'],\n","                x['score'],\n","                x['scored_by'],\n","                x['members'],\n","                x['favorites'],\n","                x['synopsis'],\n","                producer_name if x['producers'] else '',\n","                studios_name if x['studios'] else '',\n","                genres_name if x['genres'] else '',\n","                themes_name if x['themes'] else '',\n","                demographics_name if x['demographics'] else ''\n","                ]\n","      ourdata.append(content)\n","          \n","# Lets wait as too many request might overload the API          \n","wait= 1\n","def scrape_db():\n","  last_page = requests.get(api_url).json()['pagination']['last_visible_page']\n","  print('Started:', datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n","\n","  for page in tqdm.trange(1,last_page+1):\n","    start = time.perf_counter()\n","    get_data(page)\n","    end = time.perf_counter()\n","    time.sleep(max(0, start + wait - end))\n","\n","  print('Finished:', datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n","\n","\n","# Function to run the code to fetch our data\n","scrape_db()"],"metadata":{"id":"50dgrzCJbuEE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678463973674,"user_tz":-480,"elapsed":890783,"user":{"displayName":"Kuro Neko","userId":"15297484354292566686"}},"outputId":"9d18a8f5-6cce-4059-a92e-1da44f4c7368"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Started: 2023-03-10 15:44:43\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 888/888 [14:50<00:00,  1.00s/it]"]},{"output_type":"stream","name":"stdout","text":["Finished: 2023-03-10 15:59:33\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["# This will write the data as well as the csvheader to a csv file,I have saved it under my google drive but feel free to change as per the situation\n","import csv\n","with open('/content/drive/MyDrive/Anime_Recommender/animelist.csv','w',encoding = 'UTF8',newline='') as f:\n","  writer = csv.writer(f)\n","  writer.writerow(csvheader)\n","  writer.writerows(ourdata)"],"metadata":{"id":"4NRftBe0bo1L","executionInfo":{"status":"ok","timestamp":1678464062397,"user_tz":-480,"elapsed":636,"user":{"displayName":"Kuro Neko","userId":"15297484354292566686"}}},"execution_count":7,"outputs":[]}]}